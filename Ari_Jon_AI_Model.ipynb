{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15Duh72n8Il2LxJ7zTXvbSmgDryzAJsHy",
      "authorship_tag": "ABX9TyNQAwexOc1PmOb/JClyAF6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USAFADFCS/final-project-jon-and-ari/blob/main/Ari_Jon_AI_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Save to google drive"
      ],
      "metadata": {
        "id": "nqBzLcXj1b9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c9QrwGLv1bKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "124t0OBDzFFY",
        "outputId": "255a5e14-b177-4931-f554-d73c4e3e1933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# asr_whisper.py\n",
        "import os, torch\n",
        "from transformers import pipeline\n",
        "\n",
        "MODEL_ID = \"openai/whisper-large-v3-turbo\"\n",
        "DEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Build one reusable pipeline\n",
        "asr = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=MODEL_ID,\n",
        "    device=DEVICE,\n",
        "    torch_dtype=DTYPE,\n",
        "    chunk_length_s=30,           # robust for long audio\n",
        "    stride_length_s=5,           # overlap for context\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "MEDICAL_LEXICON = (\n",
        "    \"tourniquet, hemorrhage, capillary refill, obey commands, airway, \"\n",
        "    \"respirations, pulse, radial pulse, naloxone, unresponsive, shock\"\n",
        ")\n",
        "\n",
        "def transcribe(path: str) -> dict:\n",
        "    return asr(\n",
        "        path,\n",
        "        generate_kwargs={\n",
        "            \"task\": \"transcribe\",      # or \"translate\" if needed\n",
        "            \"temperature\": 0.0,\n",
        "            \"num_beams\": 5\n",
        "        },\n",
        "        # primes decoding with triage vocabulary\n",
        "        prompt=MEDICAL_LEXICON,\n",
        "        return_timestamps=True\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M55h80V3zLiJ",
        "outputId": "303bb52b-0ea4-4a2a-b1f3-ccb190615928"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vad.py\n",
        "import torchaudio\n",
        "\n",
        "def simple_vad_chunks(wav_path, min_speech_len=0.6):\n",
        "    wav, sr = torchaudio.load(wav_path)\n",
        "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "    vad = torchaudio.transforms.Vad(sample_rate=16000)\n",
        "    voiced = vad(wav.squeeze(0))\n",
        "    # Fallback: if overly aggressive, just return original path\n",
        "    if voiced.numel() < 16000 * min_speech_len:\n",
        "        return [wav_path]\n",
        "    # For brevity, write voiced chunk to temp file; in production, slice windows\n",
        "    out = \"/tmp/voiced.wav\"\n",
        "    torchaudio.save(out, voiced.unsqueeze(0), 16000)\n",
        "    return [out]\n"
      ],
      "metadata": {
        "id": "ao0HDL4TzXNQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24b36519",
        "outputId": "1eec9faf-a394-4a1a-fb8c-36467f9806a4"
      },
      "source": [
        "%%writefile asr_whisper.py\n",
        "import os, torch\n",
        "from transformers import pipeline\n",
        "\n",
        "MODEL_ID = \"openai/whisper-large-v3-turbo\"\n",
        "DEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Build one reusable pipeline\n",
        "asr = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=MODEL_ID,\n",
        "    device=DEVICE,\n",
        "    torch_dtype=DTYPE,\n",
        "    chunk_length_s=30,           # robust for long audio\n",
        "    stride_length_s=5,           # overlap for context\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "MEDICAL_LEXICON = (\n",
        "    \"tourniquet, hemorrhage, capillary refill, obey commands, airway, \"\n",
        "    \"respirations, pulse, radial pulse, naloxone, unresponsive, shock\"\n",
        ")\n",
        "\n",
        "def transcribe(path: str) -> dict:\n",
        "    return asr(\n",
        "        path,\n",
        "        generate_kwargs={\n",
        "            \"task\": \"transcribe\",      # or \"translate\" if needed\n",
        "            \"temperature\": 0.0,\n",
        "            \"num_beams\": 5\n",
        "        },\n",
        "        # primes decoding with triage vocabulary\n",
        "        prompt=MEDICAL_LEXICON,\n",
        "        return_timestamps=True\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing asr_whisper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f51f5c7c",
        "outputId": "dd022244-8b16-4054-db47-8ded559ca553"
      },
      "source": [
        "%%writefile vad.py\n",
        "import torchaudio\n",
        "\n",
        "def simple_vad_chunks(wav_path, min_speech_len=0.6):\n",
        "    wav, sr = torchaudio.load(wav_path)\n",
        "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "    vad = torchaudio.transforms.Vad(sample_rate=16000)\n",
        "    voiced = vad(wav.squeeze(0))\n",
        "    # Fallback: if overly aggressive, just return original path\n",
        "    if voiced.numel() < 16000 * min_speech_len:\n",
        "        return [wav_path]\n",
        "    # For brevity, write voiced chunk to temp file; in production, slice windows\n",
        "    out = \"/tmp/voiced.wav\"\n",
        "    torchaudio.save(out, voiced.unsqueeze(0), 16000)\n",
        "    return [out]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vad.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2691ee3d",
        "outputId": "522fcac5-d8af-46f2-fead-54d8052d1c56"
      },
      "source": [
        "# main.py\n",
        "from asr_whisper import transcribe\n",
        "from vad import simple_vad_chunks\n",
        "\n",
        "def transcribe_with_vad(path):\n",
        "    out = {\"text\": \"\", \"segments\": []}\n",
        "    for chunk in simple_vad_chunks(path):\n",
        "        r = transcribe(chunk)\n",
        "        out[\"text\"] += (\" \" + r[\"text\"]).strip()\n",
        "        if \"chunks\" in r:\n",
        "            out[\"segments\"].extend(r[\"chunks\"])\n",
        "    return out"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGE_SCHEMA = {\n",
        "  \"patient_id\": str,\n",
        "  \"entities\": {\n",
        "    \"bleeding_severe\": bool,\n",
        "    \"can_walk\": bool | None,\n",
        "    \"obeys_commands\": bool | None,\n",
        "    \"resp_rate\": int | None,\n",
        "    \"cap_refill_sec\": float | None,\n",
        "    \"mental_status\": str | None\n",
        "  },\n",
        "  \"evidence\": list,              # text snippets / timestamps\n",
        "  \"triage_candidate\": str,       # Immediate | Delayed | Minimal | Expectant | Unknown\n",
        "  \"uncertainty\": float,\n",
        "  \"next_question\": str | None\n",
        "}\n"
      ],
      "metadata": {
        "id": "1wAmgcIE0qrE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def salt_rules(e, sensors=None):\n",
        "    s = sensors or {}\n",
        "    severe_bleed = e.get(\"bleeding_severe\") or s.get(\"bleeding_detected\")\n",
        "    resp = e.get(\"resp_rate\") or s.get(\"resp_rate\")\n",
        "    obeys = e.get(\"obeys_commands\")\n",
        "    can_walk = e.get(\"can_walk\")\n",
        "\n",
        "    if can_walk is True:\n",
        "        return \"Minimal\"\n",
        "    if severe_bleed:\n",
        "        return \"Immediate\"\n",
        "    if resp is None:\n",
        "        return \"Unknown\"\n",
        "    if resp == 0:\n",
        "        return \"Expectant\"\n",
        "    if obeys is False or (resp and resp >= 30):\n",
        "        return \"Immediate\"\n",
        "    return \"Delayed\"\n"
      ],
      "metadata": {
        "id": "WEqWjgrc1sY2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoProcessor\n",
        "from asr_whisper import asr, MEDICAL_LEXICON  # your pipeline + vocab\n",
        "\n",
        "print(\"Device:\", asr.device)\n",
        "print(\"Torch dtype:\", next(asr.model.parameters()).dtype)\n",
        "print(\"Task:\", asr.task)\n",
        "\n",
        "MODEL_ID = asr.model.name_or_path\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Build prompt ids and coerce to a Torch tensor on the right device\n",
        "prompt_ids = processor.get_prompt_ids(text=MEDICAL_LEXICON)\n",
        "\n",
        "# handle list/np.array return types robustly\n",
        "if isinstance(prompt_ids, np.ndarray):\n",
        "    prompt_ids = prompt_ids.tolist()\n",
        "elif isinstance(prompt_ids, tuple):\n",
        "    prompt_ids = list(prompt_ids)\n",
        "\n",
        "prompt_ids = torch.tensor(prompt_ids, dtype=torch.long, device=asr.model.device)\n",
        "\n",
        "AUDIO = \"JapaneseTest.mp3\"  # <-- ensure this exists\n",
        "\n",
        "# If your model’s generation_config had old forced ids set, clear them:\n",
        "try:\n",
        "    asr.model.generation_config.forced_decoder_ids = None\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "r = asr(\n",
        "    AUDIO,\n",
        "    generate_kwargs={\n",
        "        \"language\": \"en\",          # prefer flags over forced_decoder_ids\n",
        "        \"task\": \"transcribe\",\n",
        "        \"prompt_ids\": prompt_ids,  # <-- now a torch.tensor\n",
        "        \"temperature\": 0.0,\n",
        "        \"num_beams\": 5,\n",
        "        \"do_sample\": False,\n",
        "    },\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "print(\"Output keys:\", list(r.keys()))\n",
        "print(\"Text (first 120):\", r.get(\"text\", \"\")[:120])\n",
        "print(\"Num segments:\", len(r.get(\"chunks\", [])))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqLDtYu9340T",
        "outputId": "15d98869-ac2d-4965-ddae-9d064d342adb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Torch dtype: torch.float32\n",
            "Task: automatic-speech-recognition\n",
            "Output keys: ['text', 'chunks']\n",
            "Text (first 120):  こんにちは。私の名前はワットソンジャノソンです。私は少し日本語ができます。よろしくお願いします。\n",
            "Num segments: 1\n"
          ]
        }
      ]
    }
  ]
}