{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USAFADFCS/final-project-jon-and-ari/blob/main/Ari_Jon_AI_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqBzLcXj1b9K"
      },
      "source": [
        "  Save to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "124t0OBDzFFY",
        "outputId": "255a5e14-b177-4931-f554-d73c4e3e1933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup- Import Libraries and Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M55h80V3zLiJ",
        "outputId": "303bb52b-0ea4-4a2a-b1f3-ccb190615928"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, torch, torchaudio\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from asr_whisper import transcribe\n",
        "from vad import simple_vad_chunks\n",
        "from transformers import AutoProcessor\n",
        "from asr_whisper import asr, MEDICAL_LEXICON  # your pipeline + vocab\n",
        "\n",
        "MODEL_ID = \"openai/whisper-tiny.en\"\n",
        "DEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Build one reusable pipeline\n",
        "asr = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=MODEL_ID,\n",
        "    device=DEVICE,\n",
        "    torch_dtype=DTYPE,\n",
        "    chunk_length_s=30,           # robust for long audio\n",
        "    stride_length_s=5,           # overlap for context\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "MEDICAL_LEXICON = (\n",
        "    \"tourniquet, hemorrhage, capillary refill, obey commands, airway, \"\n",
        "    \"respirations, pulse, radial pulse, naloxone, unresponsive, shock\"\n",
        ")\n",
        "\n",
        "def transcribe(path: str) -> dict:\n",
        "    return asr(\n",
        "        path,\n",
        "        generate_kwargs={\n",
        "            \"task\": \"transcribe\",      # or \"translate\" if needed\n",
        "            \"temperature\": 0.0,\n",
        "            \"num_beams\": 5\n",
        "        },\n",
        "        # primes decoding with triage vocabulary\n",
        "        prompt=MEDICAL_LEXICON,\n",
        "        return_timestamps=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao0HDL4TzXNQ"
      },
      "outputs": [],
      "source": [
        "def simple_vad_chunks(wav_path, min_speech_len=0.6):\n",
        "    wav, sr = torchaudio.load(wav_path)\n",
        "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "    vad = torchaudio.transforms.Vad(sample_rate=16000)\n",
        "    voiced = vad(wav.squeeze(0))\n",
        "    # Fallback: if overly aggressive, just return original path\n",
        "    if voiced.numel() < 16000 * min_speech_len:\n",
        "        return [wav_path]\n",
        "    # For brevity, write voiced chunk to temp file; in production, slice windows\n",
        "    out = \"/tmp/voiced.wav\"\n",
        "    torchaudio.save(out, voiced.unsqueeze(0), 16000)\n",
        "    return [out]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2691ee3d",
        "outputId": "522fcac5-d8af-46f2-fead-54d8052d1c56"
      },
      "outputs": [],
      "source": [
        "def transcribe_with_vad(path):\n",
        "    out = {\"text\": \"\", \"segments\": []}\n",
        "    for chunk in simple_vad_chunks(path):\n",
        "        r = transcribe(chunk)\n",
        "        out[\"text\"] += (\" \" + r[\"text\"]).strip()\n",
        "        if \"chunks\" in r:\n",
        "            out[\"segments\"].extend(r[\"chunks\"])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wAmgcIE0qrE"
      },
      "outputs": [],
      "source": [
        "TRIAGE_SCHEMA = {\n",
        "  \"patient_id\": str,\n",
        "  \"entities\": {\n",
        "    \"bleeding_severe\": bool,\n",
        "    \"can_walk\": bool | None,\n",
        "    \"obeys_commands\": bool | None,\n",
        "    \"resp_rate\": int | None,\n",
        "    \"cap_refill_sec\": float | None,\n",
        "    \"mental_status\": str | None\n",
        "  },\n",
        "  \"evidence\": list,              # text snippets / timestamps\n",
        "  \"triage_candidate\": str,       # Immediate | Delayed | Minimal | Expectant | Unknown\n",
        "  \"uncertainty\": float,\n",
        "  \"next_question\": str | None\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEqWjgrc1sY2"
      },
      "outputs": [],
      "source": [
        "def salt_rules(e, sensors=None):\n",
        "    s = sensors or {}\n",
        "    severe_bleed = e.get(\"bleeding_severe\") or s.get(\"bleeding_detected\")\n",
        "    resp = e.get(\"resp_rate\") or s.get(\"resp_rate\")\n",
        "    obeys = e.get(\"obeys_commands\")\n",
        "    can_walk = e.get(\"can_walk\")\n",
        "\n",
        "    if can_walk is True:\n",
        "        return \"Minimal\"\n",
        "    if severe_bleed:\n",
        "        return \"Immediate\"\n",
        "    if resp is None:\n",
        "        return \"Unknown\"\n",
        "    if resp == 0:\n",
        "        return \"Expectant\"\n",
        "    if obeys is False or (resp and resp >= 30):\n",
        "        return \"Immediate\"\n",
        "    return \"Delayed\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqLDtYu9340T",
        "outputId": "15d98869-ac2d-4965-ddae-9d064d342adb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Torch dtype: torch.float32\n",
            "Task: automatic-speech-recognition\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "print(\"Device:\", asr.device)\n",
        "print(\"Torch dtype:\", next(asr.model.parameters()).dtype)\n",
        "print(\"Task:\", asr.task)\n",
        "\n",
        "MODEL_ID = asr.model.name_or_path\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Build prompt ids and coerce to a Torch tensor on the right device\n",
        "prompt_ids = processor.get_prompt_ids(text=MEDICAL_LEXICON)\n",
        "\n",
        "# handle list/np.array return types robustly\n",
        "if isinstance(prompt_ids, np.ndarray):\n",
        "    prompt_ids = prompt_ids.tolist()\n",
        "elif isinstance(prompt_ids, tuple):\n",
        "    prompt_ids = list(prompt_ids)\n",
        "\n",
        "prompt_ids = torch.tensor(prompt_ids, dtype=torch.long, device=asr.model.device)\n",
        "\n",
        "AUDIO = \"EnglishTriageTest 1.mp3\"  # <-- ensure this exists\n",
        "\n",
        "# If your modelâ€™s generation_config had old forced ids set, clear them:\n",
        "try:\n",
        "    asr.model.generation_config.forced_decoder_ids = None\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "r = asr(\n",
        "    AUDIO,\n",
        "    generate_kwargs={\n",
        "       # \"language\": \"en\",          # prefer flags over forced_decoder_ids\n",
        "       # \"task\": \"transcribe\",\n",
        "        \"prompt_ids\": prompt_ids,  # <-- now a torch.tensor\n",
        "        \"temperature\": 0.0,\n",
        "        \"num_beams\": 5,\n",
        "        \"do_sample\": False,\n",
        "    },\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "print(\"Output keys:\", list(r.keys()))\n",
        "print(\"Text (first 120):\", r.get(\"text\", \"\")[:120])\n",
        "print(\"Num segments:\", len(r.get(\"chunks\", [])))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNQAwexOc1PmOb/JClyAF6N",
      "include_colab_link": true,
      "mount_file_id": "15Duh72n8Il2LxJ7zTXvbSmgDryzAJsHy",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
