{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15Duh72n8Il2LxJ7zTXvbSmgDryzAJsHy",
      "authorship_tag": "ABX9TyPIn8KWA0QFn6THJ0rs6u4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USAFADFCS/final-project-jon-and-ari/blob/main/Ari_Jon_AI_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "124t0OBDzFFY",
        "outputId": "255a5e14-b177-4931-f554-d73c4e3e1933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# asr_whisper.py\n",
        "import os, torch\n",
        "from transformers import pipeline\n",
        "\n",
        "MODEL_ID = \"openai/whisper-large-v3-turbo\"\n",
        "DEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Build one reusable pipeline\n",
        "asr = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=MODEL_ID,\n",
        "    device=DEVICE,\n",
        "    torch_dtype=DTYPE,\n",
        "    chunk_length_s=30,           # robust for long audio\n",
        "    stride_length_s=5,           # overlap for context\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "MEDICAL_LEXICON = (\n",
        "    \"tourniquet, hemorrhage, capillary refill, obey commands, airway, \"\n",
        "    \"respirations, pulse, radial pulse, naloxone, unresponsive, shock\"\n",
        ")\n",
        "\n",
        "def transcribe(path: str) -> dict:\n",
        "    return asr(\n",
        "        path,\n",
        "        generate_kwargs={\n",
        "            \"task\": \"transcribe\",      # or \"translate\" if needed\n",
        "            \"temperature\": 0.0,\n",
        "            \"num_beams\": 5\n",
        "        },\n",
        "        # primes decoding with triage vocabulary\n",
        "        prompt=MEDICAL_LEXICON,\n",
        "        return_timestamps=True\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M55h80V3zLiJ",
        "outputId": "303bb52b-0ea4-4a2a-b1f3-ccb190615928"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vad.py\n",
        "import torchaudio\n",
        "\n",
        "def simple_vad_chunks(wav_path, min_speech_len=0.6):\n",
        "    wav, sr = torchaudio.load(wav_path)\n",
        "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "    vad = torchaudio.transforms.Vad(sample_rate=16000)\n",
        "    voiced = vad(wav.squeeze(0))\n",
        "    # Fallback: if overly aggressive, just return original path\n",
        "    if voiced.numel() < 16000 * min_speech_len:\n",
        "        return [wav_path]\n",
        "    # For brevity, write voiced chunk to temp file; in production, slice windows\n",
        "    out = \"/tmp/voiced.wav\"\n",
        "    torchaudio.save(out, voiced.unsqueeze(0), 16000)\n",
        "    return [out]\n"
      ],
      "metadata": {
        "id": "ao0HDL4TzXNQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24b36519",
        "outputId": "1eec9faf-a394-4a1a-fb8c-36467f9806a4"
      },
      "source": [
        "%%writefile asr_whisper.py\n",
        "import os, torch\n",
        "from transformers import pipeline\n",
        "\n",
        "MODEL_ID = \"openai/whisper-large-v3-turbo\"\n",
        "DEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Build one reusable pipeline\n",
        "asr = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=MODEL_ID,\n",
        "    device=DEVICE,\n",
        "    torch_dtype=DTYPE,\n",
        "    chunk_length_s=30,           # robust for long audio\n",
        "    stride_length_s=5,           # overlap for context\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "MEDICAL_LEXICON = (\n",
        "    \"tourniquet, hemorrhage, capillary refill, obey commands, airway, \"\n",
        "    \"respirations, pulse, radial pulse, naloxone, unresponsive, shock\"\n",
        ")\n",
        "\n",
        "def transcribe(path: str) -> dict:\n",
        "    return asr(\n",
        "        path,\n",
        "        generate_kwargs={\n",
        "            \"task\": \"transcribe\",      # or \"translate\" if needed\n",
        "            \"temperature\": 0.0,\n",
        "            \"num_beams\": 5\n",
        "        },\n",
        "        # primes decoding with triage vocabulary\n",
        "        prompt=MEDICAL_LEXICON,\n",
        "        return_timestamps=True\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing asr_whisper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f51f5c7c",
        "outputId": "dd022244-8b16-4054-db47-8ded559ca553"
      },
      "source": [
        "%%writefile vad.py\n",
        "import torchaudio\n",
        "\n",
        "def simple_vad_chunks(wav_path, min_speech_len=0.6):\n",
        "    wav, sr = torchaudio.load(wav_path)\n",
        "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "    vad = torchaudio.transforms.Vad(sample_rate=16000)\n",
        "    voiced = vad(wav.squeeze(0))\n",
        "    # Fallback: if overly aggressive, just return original path\n",
        "    if voiced.numel() < 16000 * min_speech_len:\n",
        "        return [wav_path]\n",
        "    # For brevity, write voiced chunk to temp file; in production, slice windows\n",
        "    out = \"/tmp/voiced.wav\"\n",
        "    torchaudio.save(out, voiced.unsqueeze(0), 16000)\n",
        "    return [out]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vad.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2691ee3d",
        "outputId": "522fcac5-d8af-46f2-fead-54d8052d1c56"
      },
      "source": [
        "# main.py\n",
        "from asr_whisper import transcribe\n",
        "from vad import simple_vad_chunks\n",
        "\n",
        "def transcribe_with_vad(path):\n",
        "    out = {\"text\": \"\", \"segments\": []}\n",
        "    for chunk in simple_vad_chunks(path):\n",
        "        r = transcribe(chunk)\n",
        "        out[\"text\"] += (\" \" + r[\"text\"]).strip()\n",
        "        if \"chunks\" in r:\n",
        "            out[\"segments\"].extend(r[\"chunks\"])\n",
        "    return out"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGE_SCHEMA = {\n",
        "  \"patient_id\": str,\n",
        "  \"entities\": {\n",
        "    \"bleeding_severe\": bool,\n",
        "    \"can_walk\": bool | None,\n",
        "    \"obeys_commands\": bool | None,\n",
        "    \"resp_rate\": int | None,\n",
        "    \"cap_refill_sec\": float | None,\n",
        "    \"mental_status\": str | None\n",
        "  },\n",
        "  \"evidence\": list,              # text snippets / timestamps\n",
        "  \"triage_candidate\": str,       # Immediate | Delayed | Minimal | Expectant | Unknown\n",
        "  \"uncertainty\": float,\n",
        "  \"next_question\": str | None\n",
        "}\n"
      ],
      "metadata": {
        "id": "1wAmgcIE0qrE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Fy-ylFsP1Fh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}