{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a9b13b",
   "metadata": {},
   "source": [
    "# Agentic Model: RATS AI Triage Classifier\n",
    "\n",
    "Combat Triage AI - Complete Implementation with Quantization and SALT Protocol\n",
    "\n",
    "Ready for Raspberry Pi Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603449cd",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1b162",
   "metadata": {},
   "source": [
    "#### Step 1A: Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f0a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (4.55.4)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.4\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.4:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.4\n",
      "\u001b[2K  Attempting uninstall: transformers‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: transformers 4.55.42m0/2\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-4.55.4:‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.55.4‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "optimum-onnx 0.0.3 requires transformers<4.56.0,>=4.36.0, but you have transformers 4.57.1 which is incompatible.\n",
      "fair-llm 0.1.0 requires huggingface-hub==0.33.0, but you have huggingface-hub 0.35.3 which is incompatible.\n",
      "fair-llm 0.1.0 requires tokenizers==0.21.1, but you have tokenizers 0.22.1 which is incompatible.\n",
      "fair-llm 0.1.0 requires transformers==4.52.4, but you have transformers 4.57.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.22.1 transformers-4.57.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: optimum[onnxruntime] in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (2.0.0)\n",
      "Requirement already satisfied: transformers>=4.29 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum[onnxruntime]) (4.57.1)\n",
      "Requirement already satisfied: torch>=1.11 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum[onnxruntime]) (2.7.1)\n",
      "Requirement already satisfied: packaging in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum[onnxruntime]) (25.0)\n",
      "Requirement already satisfied: numpy in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum[onnxruntime]) (2.3.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.8.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum[onnxruntime]) (0.35.3)\n",
      "Requirement already satisfied: optimum-onnx[onnxruntime] in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum[onnxruntime]) (0.0.3)\n",
      "Requirement already satisfied: filelock in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.1.4)\n",
      "Requirement already satisfied: setuptools in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch>=1.11->optimum[onnxruntime]) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11->optimum[onnxruntime]) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers>=4.29->optimum[onnxruntime]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers>=4.29->optimum[onnxruntime]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from transformers>=4.29->optimum[onnxruntime]) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.2)\n",
      "Collecting transformers>=4.29 (from optimum[onnxruntime])\n",
      "  Using cached transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: onnx in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.19.1)\n",
      "Requirement already satisfied: onnxruntime>=1.18.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.22.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.29->optimum[onnxruntime])\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: coloredlogs in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (5.29.5)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (10.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from onnx->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.6.15)\n",
      "Using cached transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.57.1\n",
      "\u001b[2K    Uninstalling transformers-4.57.1:‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fair-llm 0.1.0 requires huggingface-hub==0.33.0, but you have huggingface-hub 0.35.3 which is incompatible.\n",
      "fair-llm 0.1.0 requires tokenizers==0.21.1, but you have tokenizers 0.21.4 which is incompatible.\n",
      "fair-llm 0.1.0 requires transformers==4.52.4, but you have transformers 4.55.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.21.4 transformers-4.55.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchaudio in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install optimum[onnxruntime]\n",
    "!pip install torch torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5045455",
   "metadata": {},
   "source": [
    "#### Step 1B: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabfeff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aripontoni/CompSci 471/cs471-pex4/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, torch, torchaudio\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from transformers import pipeline, WhisperForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0c033",
   "metadata": {},
   "source": [
    "#### Step 1C: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be3b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"openai/whisper-tiny.en\"\n",
    "DEVICE = \"cpu\"  # Pi doesn't have GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767eadc6",
   "metadata": {},
   "source": [
    "## Step 2: Medical Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0360e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded combat medical vocabulary\n",
    "COMBAT_MEDICAL_LEXICON = \"\"\"\n",
    "tourniquet, hemorrhage, massive hemorrhage, capillary refill, \n",
    "obey commands, airway patent, airway obstructed, \n",
    "respirations per minute, respiratory rate, breathing adequately,\n",
    "radial pulse present, radial pulse absent, carotid pulse,\n",
    "shock, hypotensive, pale, clammy, cold,\n",
    "GSW, gunshot wound, blast injury, shrapnel, amputation,\n",
    "conscious, unconscious, alert, verbal, pain, unresponsive,\n",
    "chest seal, needle decompression, nasopharyngeal airway,\n",
    "combat gauze, hemostatic agent, pressure dressing,\n",
    "walking wounded, litter urgent, urgent surgical,\n",
    "can walk, cannot walk, ambulatory, unable to walk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b152e",
   "metadata": {},
   "source": [
    "## Step 3: Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739a2101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and quantizing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and quantized successfully\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========== MODEL LOADING WITH QUANTIZATION ==========\n",
    "print(\"Loading and quantizing model...\")\n",
    "\n",
    "# Load model and processor separately\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Quantize the model (makes it 4x smaller and faster)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear},  # Quantize linear layers\n",
    "    dtype=torch.qint8   # Use 8-bit integers\n",
    ")\n",
    "\n",
    "# Create ASR pipeline with quantized model AND processor components\n",
    "asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=quantized_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    device=DEVICE,\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=5,\n",
    "    return_timestamps=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded and quantized successfully\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ee3c9",
   "metadata": {},
   "source": [
    "### Step 3: Audio Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4342fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_combat_audio(audio_path):\n",
    "    \"\"\"Handle noisy battlefield audio conditions\"\"\"\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample to 16kHz (Whisper requirement)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "    \n",
    "    # Noise reduction - high-pass filter for wind/vehicle noise\n",
    "    wav = torchaudio.functional.highpass_biquad(wav, 16000, cutoff_freq=200)\n",
    "    \n",
    "    # Normalize volume (gunfire may cause clipping)\n",
    "    max_val = wav.abs().max()\n",
    "    if max_val > 0:\n",
    "        wav = wav / max_val\n",
    "    \n",
    "    return wav, 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859fecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_vad_chunks(wav_path, min_speech_len=0.6):\n",
    "    \"\"\"Voice Activity Detection - remove silence\"\"\"\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    \n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    \n",
    "    # Simple energy-based VAD\n",
    "    energy = wav.pow(2).mean(dim=0)\n",
    "    threshold = energy.mean() * 0.3\n",
    "    \n",
    "    voiced_mask = energy > threshold\n",
    "    if voiced_mask.sum() < 16000 * min_speech_len:\n",
    "        return [wav_path]  # Too short, return original\n",
    "    \n",
    "    # For simplicity, return original if has enough speech\n",
    "    return [wav_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ea1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRANSCRIPTION ==========\n",
    "def transcribe(path: str) -> dict:\n",
    "    \"\"\"Transcribe audio with medical vocabulary priming\"\"\"\n",
    "    \n",
    "    # Build prompt for medical context\n",
    "    prompt_ids = processor.get_prompt_ids(text=COMBAT_MEDICAL_LEXICON)\n",
    "    \n",
    "    if isinstance(prompt_ids, np.ndarray):\n",
    "        prompt_ids = prompt_ids.tolist()\n",
    "    elif isinstance(prompt_ids, tuple):\n",
    "        prompt_ids = list(prompt_ids)\n",
    "    \n",
    "    prompt_ids = torch.tensor(prompt_ids, dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    # Clear any old forced decoder IDs\n",
    "    try:\n",
    "        asr.model.generation_config.forced_decoder_ids = None\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    result = asr(\n",
    "        path,\n",
    "        generate_kwargs={\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"temperature\": 0.0,\n",
    "            \"num_beams\": 5,\n",
    "            \"do_sample\": False,\n",
    "        },\n",
    "        return_timestamps=True\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def transcribe_with_vad(path):\n",
    "    \"\"\"Transcribe with voice activity detection\"\"\"\n",
    "    out = {\"text\": \"\", \"chunks\": []}\n",
    "    \n",
    "    for chunk in simple_vad_chunks(path):\n",
    "        r = transcribe(chunk)\n",
    "        out[\"text\"] += (\" \" + r[\"text\"]).strip()\n",
    "        if \"chunks\" in r:\n",
    "            out[\"chunks\"].extend(r[\"chunks\"])\n",
    "    \n",
    "    return out\n",
    "\n",
    "# ========== ENTITY EXTRACTION ==========\n",
    "def extract_triage_entities(transcription_text):\n",
    "    \"\"\"Extract SALT-relevant medical information from transcription\"\"\"\n",
    "    text_lower = transcription_text.lower()\n",
    "    \n",
    "    entities = {\n",
    "        \"can_walk\": None,\n",
    "        \"bleeding_severe\": False,\n",
    "        \"obeys_commands\": None,\n",
    "        \"resp_rate\": None,\n",
    "        \"radial_pulse\": None,\n",
    "        \"mental_status\": None,\n",
    "        \"cap_refill_sec\": None\n",
    "    }\n",
    "    \n",
    "    evidence = []\n",
    "    \n",
    "    \n",
    "    # Walking ability\n",
    "    walk_yes = [\"can walk\", \"walking\", \"ambulatory\", \"able to walk\"]\n",
    "    walk_no = [\"cannot walk\", \"can't walk\", \"unable to walk\", \"not walking\"]\n",
    "    \n",
    "    for phrase in walk_yes:\n",
    "        if phrase in text_lower:\n",
    "            entities[\"can_walk\"] = True\n",
    "            evidence.append(f\"Walking: '{phrase}' detected\")\n",
    "            break\n",
    "    \n",
    "    for phrase in walk_no:\n",
    "        if phrase in text_lower:\n",
    "            entities[\"can_walk\"] = False\n",
    "            evidence.append(f\"Not walking: '{phrase}' detected\")\n",
    "            break\n",
    "    \n",
    "    # Severe bleeding\n",
    "    bleeding_phrases = [\"severe bleeding\", \"hemorrhage\", \"massive hemorrhage\", \n",
    "                       \"tourniquet applied\", \"massive bleeding\", \"heavy bleeding\"]\n",
    "    for phrase in bleeding_phrases:\n",
    "        if phrase in text_lower:\n",
    "            entities[\"bleeding_severe\"] = True\n",
    "            evidence.append(f\"Severe bleeding: '{phrase}' detected\")\n",
    "            break\n",
    "    \n",
    "    # Command response\n",
    "    obey_yes = [\"obeys commands\", \"follows commands\", \"responsive to commands\", \"responding\"]\n",
    "    obey_no = [\"does not obey\", \"doesn't obey\", \"unresponsive\", \"no response\", \n",
    "               \"not responding\", \"not obeying\"]\n",
    "    \n",
    "    for phrase in obey_yes:\n",
    "        if phrase in text_lower:\n",
    "            entities[\"obeys_commands\"] = True\n",
    "            evidence.append(f\"Obeys commands: '{phrase}' detected\")\n",
    "            break\n",
    "    \n",
    "    for phrase in obey_no:\n",
    "        if phrase in text_lower:\n",
    "            entities[\"obeys_commands\"] = False\n",
    "            evidence.append(f\"Does not obey: '{phrase}' detected\")\n",
    "            break\n",
    "    \n",
    "    # Respiratory rate extraction\n",
    "    resp_patterns = [\n",
    "        r'(\\d+)\\s*breaths?\\s*(?:per\\s*minute)?',\n",
    "        r'(\\d+)\\s*respirations?\\s*(?:per\\s*minute)?',\n",
    "        r'respiratory\\s*rate\\s*(?:of\\s*)?(\\d+)',\n",
    "        r'breathing\\s*(?:at\\s*)?(\\d+)',\n",
    "        r'(\\d+)\\s*rpm'\n",
    "    ]\n",
    "    \n",
    "    for pattern in resp_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            entities[\"resp_rate\"] = int(match.group(1))\n",
    "            evidence.append(f\"Respiratory rate: {match.group(1)} detected\")\n",
    "            break\n",
    "    \n",
    "    # Radial pulse\n",
    "    pulse_yes = [\"radial pulse present\", \"has radial pulse\", \"pulse present\"]\n",
    "    pulse_no = [\"no radial pulse\", \"radial pulse absent\", \"no pulse\"]\n",
    "    \n",
    "    for phrase in pulse_yes:\n",
    "        if phrase in text_lower:\n",
    "            entities[\"radial_pulse\"] = True\n",
    "            evidence.append(f\"Radial pulse: '{phrase}' detected\")\n",
    "            break\n",
    "    \n",
    "    for phrase in pulse_no:\n",
    "        if phrase in text_lower:\n",
    "            entities[\"radial_pulse\"] = False\n",
    "            evidence.append(f\"No radial pulse: '{phrase}' detected\")\n",
    "            break\n",
    "    \n",
    "    # Mental status\n",
    "    if \"alert\" in text_lower:\n",
    "        entities[\"mental_status\"] = \"alert\"\n",
    "        evidence.append(\"Mental status: alert\")\n",
    "    elif \"verbal\" in text_lower or \"responds to verbal\" in text_lower:\n",
    "        entities[\"mental_status\"] = \"verbal\"\n",
    "        evidence.append(\"Mental status: verbal\")\n",
    "    elif \"pain\" in text_lower or \"responds to pain\" in text_lower:\n",
    "        entities[\"mental_status\"] = \"pain\"\n",
    "        evidence.append(\"Mental status: pain\")\n",
    "    elif \"unresponsive\" in text_lower:\n",
    "        entities[\"mental_status\"] = \"unresponsive\"\n",
    "        evidence.append(\"Mental status: unresponsive\")\n",
    "    \n",
    "    return entities, evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27d05e",
   "metadata": {},
   "source": [
    "## Step 5: Implement SALT Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a22e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SALT TRIAGE RULES ==========\n",
    "def salt_rules(entities, sensors=None):\n",
    "    \"\"\"\n",
    "    Implement SALT (Sort, Assess, Lifesaving interventions, Treatment/Transport) triage\n",
    "    \n",
    "    Categories:\n",
    "    - Immediate (Red): Life-threatening injuries, needs immediate care\n",
    "    - Delayed (Yellow): Serious injuries, can wait for treatment\n",
    "    - Minimal (Green): Minor injuries, walking wounded\n",
    "    - Expectant (Black): Injuries incompatible with life\n",
    "    \"\"\"\n",
    "    s = sensors or {}\n",
    "    \n",
    "    # Merge sensor data\n",
    "    can_walk = entities.get(\"can_walk\") or s.get(\"can_walk\")\n",
    "    severe_bleed = entities.get(\"bleeding_severe\") or s.get(\"bleeding_detected\")\n",
    "    resp = entities.get(\"resp_rate\") or s.get(\"resp_rate\")\n",
    "    obeys = entities.get(\"obeys_commands\") or s.get(\"obeys_commands\")\n",
    "    radial_pulse = entities.get(\"radial_pulse\") or s.get(\"radial_pulse\")\n",
    "    \n",
    "    # SALT Algorithm\n",
    "    # Step 1: Can the patient walk?\n",
    "    if can_walk is True:\n",
    "        return \"Minimal\"\n",
    "    \n",
    "    # Step 2: Assess for life-threatening bleeding\n",
    "    if severe_bleed:\n",
    "        return \"Immediate\"\n",
    "    \n",
    "    # Step 3: Check respirations\n",
    "    if resp is None:\n",
    "        return \"Unknown\"  # Need more data\n",
    "    \n",
    "    if resp == 0:\n",
    "        return \"Expectant\"  # Not breathing\n",
    "    \n",
    "    if resp >= 30:\n",
    "        return \"Immediate\"  # Respiratory distress\n",
    "    \n",
    "    # Step 4: Check mental status / obeys commands\n",
    "    if obeys is False:\n",
    "        return \"Immediate\"  # Altered mental status\n",
    "    \n",
    "    # Step 5: Check radial pulse (perfusion)\n",
    "    if radial_pulse is False:\n",
    "        return \"Immediate\"  # Poor perfusion\n",
    "    \n",
    "    # Default: injuries present but stable\n",
    "    return \"Delayed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ddc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIDENCE & NEXT QUESTION ==========\n",
    "def calculate_confidence(entities):\n",
    "    \"\"\"Calculate confidence based on how much data we have\"\"\"\n",
    "    total_fields = len(entities)\n",
    "    filled_fields = sum(1 for v in entities.values() if v is not None and v is not False)\n",
    "    return filled_fields / total_fields\n",
    "\n",
    "def suggest_next_question(entities):\n",
    "    \"\"\"Ask medic for missing critical SALT info\"\"\"\n",
    "    \n",
    "    if entities[\"can_walk\"] is None:\n",
    "        return \"Can the patient walk?\"\n",
    "    \n",
    "    if not entities[\"bleeding_severe\"] and entities.get(\"bleeding_severe\") is None:\n",
    "        return \"Is there severe bleeding or hemorrhage?\"\n",
    "    \n",
    "    if entities[\"resp_rate\"] is None:\n",
    "        return \"What is the respiratory rate per minute?\"\n",
    "    \n",
    "    if entities[\"obeys_commands\"] is None:\n",
    "        return \"Does the patient obey commands?\"\n",
    "    \n",
    "    if entities[\"radial_pulse\"] is None:\n",
    "        return \"Is there a radial pulse present?\"\n",
    "    \n",
    "    return None  # All critical data collected\n",
    "\n",
    "# ========== SENSOR FUSION ==========\n",
    "def fuse_sensor_data(audio_entities, drone_sensors):\n",
    "    \"\"\"Combine voice transcription with drone sensor data\"\"\"\n",
    "    final_entities = audio_entities.copy()\n",
    "    \n",
    "    # Sensor data overrides uncertain voice data\n",
    "    if drone_sensors.get(\"thermal_bleeding_detected\") is not None:\n",
    "        if audio_entities[\"bleeding_severe\"] is None or not audio_entities[\"bleeding_severe\"]:\n",
    "            final_entities[\"bleeding_severe\"] = drone_sensors[\"thermal_bleeding_detected\"]\n",
    "    \n",
    "    if drone_sensors.get(\"movement_detected\") is not None:\n",
    "        if audio_entities[\"can_walk\"] is None:\n",
    "            final_entities[\"can_walk\"] = drone_sensors[\"movement_detected\"]\n",
    "    \n",
    "    if drone_sensors.get(\"heart_rate\") is not None:\n",
    "        # Estimate respiratory rate from heart rate if not available\n",
    "        if audio_entities[\"resp_rate\"] is None:\n",
    "            # Rough estimate: normal resp is ~1/4 of heart rate\n",
    "            final_entities[\"resp_rate\"] = int(drone_sensors[\"heart_rate\"] / 4)\n",
    "    \n",
    "    return final_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af81fc",
   "metadata": {},
   "source": [
    "## Step 6: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ca6bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 1: Audio transcription only\n",
      "\n",
      "============================================================\n",
      "TRIAGE ASSESSMENT INITIATED\n",
      "============================================================\n",
      "\n",
      "üìù Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Transcription: Ow, my leg hurts, and I can't breathe....\n",
      "\n",
      "üîç Extracting medical information...\n",
      "\n",
      "üè• Applying SALT triage protocol...\n",
      "\n",
      "============================================================\n",
      "TRIAGE RESULTS\n",
      "============================================================\n",
      "üöë Category: Unknown\n",
      "üìä Confidence: 0%\n",
      "‚è±Ô∏è  Processing Time: 4.86s\n",
      "\n",
      "üìã Extracted Information:\n",
      "  ‚Ä¢ bleeding_severe: False\n",
      "\n",
      "‚ùì Recommended Question: Can the patient walk?\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "TEST 2: Audio + sensor fusion\n",
      "\n",
      "============================================================\n",
      "TRIAGE ASSESSMENT INITIATED\n",
      "============================================================\n",
      "\n",
      "üìù Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Transcription: Ow, my leg hurts, and I can't breathe....\n",
      "\n",
      "üîç Extracting medical information...\n",
      "ü§ñ Fusing with sensor data...\n",
      "\n",
      "üè• Applying SALT triage protocol...\n",
      "\n",
      "============================================================\n",
      "TRIAGE RESULTS\n",
      "============================================================\n",
      "üöë Category: Immediate\n",
      "üìä Confidence: 14%\n",
      "‚è±Ô∏è  Processing Time: 2.32s\n",
      "\n",
      "üìã Extracted Information:\n",
      "  ‚Ä¢ can_walk: False\n",
      "  ‚Ä¢ bleeding_severe: False\n",
      "  ‚Ä¢ resp_rate: 30\n",
      "\n",
      "‚ùì Recommended Question: Does the patient obey commands?\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== COMPLETE TRIAGE PIPELINE ==========\n",
    "def triage_patient(audio_path, sensor_data=None):\n",
    "    \"\"\"\n",
    "    Complete combat triage pipeline\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file of medic assessment\n",
    "        sensor_data: Optional dict of drone sensor readings\n",
    "        \n",
    "    Returns:\n",
    "        Full triage assessment with category and confidence\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRIAGE ASSESSMENT INITIATED\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Step 1: Transcribe audio\n",
    "    print(\"üìù Transcribing audio...\")\n",
    "    transcription = transcribe_with_vad(audio_path)\n",
    "    print(f\"‚úì Transcription: {transcription['text'][:100]}...\")\n",
    "    \n",
    "    # Step 2: Extract medical entities\n",
    "    print(\"\\nüîç Extracting medical information...\")\n",
    "    entities, evidence = extract_triage_entities(transcription[\"text\"])\n",
    "    \n",
    "    # Step 3: Fuse with sensor data if available\n",
    "    if sensor_data:\n",
    "        print(\"ü§ñ Fusing with sensor data...\")\n",
    "        entities = fuse_sensor_data(entities, sensor_data)\n",
    "    \n",
    "    # Step 4: Apply SALT triage rules\n",
    "    print(\"\\nüè• Applying SALT triage protocol...\")\n",
    "    triage_category = salt_rules(entities, sensor_data)\n",
    "    \n",
    "    # Step 5: Calculate confidence and suggest next question\n",
    "    confidence = calculate_confidence(entities)\n",
    "    next_question = suggest_next_question(entities)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Format results\n",
    "    result = {\n",
    "        \"patient_id\": os.path.basename(audio_path),\n",
    "        \"transcription\": transcription[\"text\"],\n",
    "        \"entities\": entities,\n",
    "        \"evidence\": evidence,\n",
    "        \"triage_category\": triage_category,\n",
    "        \"confidence\": confidence,\n",
    "        \"next_question\": next_question,\n",
    "        \"processing_time_sec\": round(processing_time, 2),\n",
    "        \"timestamp\": transcription.get(\"chunks\", [])\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRIAGE RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üöë Category: {triage_category}\")\n",
    "    print(f\"üìä Confidence: {confidence*100:.0f}%\")\n",
    "    print(f\"‚è±Ô∏è  Processing Time: {processing_time:.2f}s\")\n",
    "    print(f\"\\nüìã Extracted Information:\")\n",
    "    for key, value in entities.items():\n",
    "        if value is not None:\n",
    "            print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "    \n",
    "    if next_question:\n",
    "        print(f\"\\n‚ùì Recommended Question: {next_question}\")\n",
    "    \n",
    "    if evidence:\n",
    "        print(f\"\\nüìù Evidence:\")\n",
    "        for item in evidence:\n",
    "            print(f\"  ‚Ä¢ {item}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ========== TESTING ==========\n",
    "# Test with your audio file\n",
    "AUDIO = \"EnglishTriageTest 1.mp3\"\n",
    "\n",
    "# Example 1: Audio only\n",
    "print(\"TEST 1: Audio transcription only\")\n",
    "result1 = triage_patient(AUDIO)\n",
    "\n",
    "# Example 2: Audio + sensor data\n",
    "print(\"\\n\\nTEST 2: Audio + sensor fusion\")\n",
    "mock_sensor_data = {\n",
    "    \"thermal_bleeding_detected\": False,\n",
    "    \"movement_detected\": False,\n",
    "    \"heart_rate\": 120\n",
    "}\n",
    "result2 = triage_patient(AUDIO, sensor_data=mock_sensor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece06fb",
   "metadata": {},
   "source": [
    "## Step 7: Performance metrics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88117c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "============================================================\n",
      "Model: openai/whisper-tiny.en\n",
      "Quantized: Yes (8-bit)\n",
      "Device: cpu\n",
      "Model parameters: 21.2M\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== PERFORMANCE METRICS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Quantized: Yes (8-bit)\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in asr.model.parameters()) / 1e6:.1f}M\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
